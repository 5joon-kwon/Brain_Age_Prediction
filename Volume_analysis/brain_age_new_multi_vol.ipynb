{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d00df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 12:58:28.217902: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-06 12:58:28.251465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-06 12:58:28.251496: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-06 12:58:28.251514: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-06 12:58:28.257645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 12:58:28.951769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from monai.transforms import *\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fede5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_model.model_files.multi_sfcn import MultiSFCN\n",
    "from dp_model import dp_loss as dpl\n",
    "from dp_model import dp_utils as dpu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf9ffcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>f</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>IXI002-Guys-0828-T1.nii.gz</td>\n",
       "      <td>35.800137</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>IXI012-HH-1211-T1.nii.gz</td>\n",
       "      <td>38.781656</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>IXI013-HH-1212-T1.nii.gz</td>\n",
       "      <td>46.710472</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IXI014-HH-1236-T1.nii.gz</td>\n",
       "      <td>34.236824</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IXI015-HH-1258-T1.nii.gz</td>\n",
       "      <td>24.284736</td>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>IXI648-Guys-1107-T1.nii.gz</td>\n",
       "      <td>47.723477</td>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>IXI651-Guys-1118-T1.nii.gz</td>\n",
       "      <td>50.395619</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>IXI652-Guys-1116-T1.nii.gz</td>\n",
       "      <td>42.989733</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>IXI653-Guys-1122-T1.nii.gz</td>\n",
       "      <td>46.220397</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>IXI662-Guys-1120-T1.nii.gz</td>\n",
       "      <td>41.741273</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                           f        age  sex  height  weight\n",
       "0      1  IXI002-Guys-0828-T1.nii.gz  35.800137    2     164      58\n",
       "1      2    IXI012-HH-1211-T1.nii.gz  38.781656    1     175      70\n",
       "2      3    IXI013-HH-1212-T1.nii.gz  46.710472    1     182      70\n",
       "3      4    IXI014-HH-1236-T1.nii.gz  34.236824    2     163      65\n",
       "4      5    IXI015-HH-1258-T1.nii.gz  24.284736    1     181      90\n",
       "..   ...                         ...        ...  ...     ...     ...\n",
       "495  496  IXI648-Guys-1107-T1.nii.gz  47.723477    1     193     120\n",
       "496  497  IXI651-Guys-1118-T1.nii.gz  50.395619    1     175      61\n",
       "497  498  IXI652-Guys-1116-T1.nii.gz  42.989733    1     163      80\n",
       "498  499  IXI653-Guys-1122-T1.nii.gz  46.220397    1     172     100\n",
       "499  500  IXI662-Guys-1120-T1.nii.gz  41.741273    1     182      98\n",
       "\n",
       "[500 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = './IXI_train.csv'\n",
    "df = pd.read_csv(csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a5158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = './IXI_train.csv'\n",
    "DATA_PATH = '/mnt/babymri/data_age_mni1mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42243cd0-fbfe-48d4-b90b-5a5054fa9347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oj/ycm'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555f2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 폴더 생성\n",
    "project_name = \"multi_sfcn\"\n",
    "data_root = '/home/oj/ycm'\n",
    "    \n",
    "# for name in subdir_li:\n",
    "#     subdir_path = f'{dir_path}/{name}'\n",
    "#     if os.path.exists(subdir_path) == False :\n",
    "#         os.mkdir(subdir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c895e95",
   "metadata": {},
   "source": [
    "## 3D_T1_reg_PD.nii.gz 있는 폴더만 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61c6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477\n"
     ]
    }
   ],
   "source": [
    "## 3D_T1_reg_PD.nii.gz가 nas에 있으면 가져오기\n",
    "\n",
    "import glob\n",
    "\n",
    "# data_pattern = '/mnt/babymri/data_age_mni1mm/*/TOF_bet.nii.gz'\n",
    "data_pattern = '/mnt/babymri/data_age_mni1mm/*/T1_aseg.nii.gz'\n",
    "\n",
    "# 해당 패턴에 매칭되는 모든 파일 경로 가져오기\n",
    "all_files = glob.glob(data_pattern, recursive=True)\n",
    "\n",
    "# '3D_T1_reg_PD.nii.gz' 파일이 있는 디렉토리만 추출\n",
    "directories_with_file = [os.path.dirname(file) for file in all_files if os.path.isfile(file)]\n",
    "\n",
    "# 결과 출력\n",
    "# for directory in directories_with_file:\n",
    "#     print(directory)\n",
    "\n",
    "print(len(directories_with_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c940d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488\n"
     ]
    }
   ],
   "source": [
    "## csv f축 가져오기\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "for idx, row in df.iterrows(): # df 각 row 돌면서 index랑 row 정보(tuple) 저장\n",
    "    filename = os.path.join(DATA_PATH, row['f'].replace('-T1.nii.gz', ''))\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        image_paths.append(filename)\n",
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01bec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "## csv와 nas에 둘다 존재하는 파일 + 3D_T1_reg_PD.nii.gz 존재\n",
    "\n",
    "exist_image = []\n",
    "for image in directories_with_file:\n",
    "    if image in image_paths:\n",
    "        exist_image.append(image)\n",
    "        \n",
    "print(len(exist_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de61505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 442 442 442\n"
     ]
    }
   ],
   "source": [
    "## image, label 가져오기\n",
    "\n",
    "image_paths = []\n",
    "tof_paths = []\n",
    "mask_paths = []\n",
    "labels = []\n",
    "for idx, row in df.iterrows(): # df 각 row 돌면서 index랑 row 정보(tuple) 저장\n",
    "    filename = os.path.join(DATA_PATH, row['f'].replace('-T1.nii.gz', ''))\n",
    "#     filename = os.path.join(DATA_PATH, row['f'].replace('-T1.nii.gz', ''), 'nifti', '3D_T1_reg_PD.nii.gz')\n",
    "    if filename in exist_image:\n",
    "        t1name = os.path.join(filename, 'T1_bet.nii.gz')\n",
    "        tofname = os.path.join(filename, 'TOF_bet.nii.gz')\n",
    "        maskname = os.path.join(filename, 'T1_aseg.nii.gz')\n",
    "        image_paths.append(t1name)\n",
    "        tof_paths.append(tofname)\n",
    "        mask_paths.append(maskname)\n",
    "        labels.append(int(row['age']))\n",
    "print(len(image_paths),len(tof_paths),len(mask_paths), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaac65f0-38b0-4f5f-9663-5f91974b3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ind = np.arange(0,len(image_paths))\n",
    "data_dict = [\n",
    "    {\n",
    "        \"t1\": image_paths[ind],\n",
    "        \"tof\": tof_paths[ind],\n",
    "        \"mask\": mask_paths[ind],\n",
    "        \"label\": labels[ind]\n",
    "    }\n",
    "    for ind in data_ind\n",
    "]\n",
    "\n",
    "train_Data = data_dict[88:]\n",
    "val_Data = data_dict[:88]\n",
    "n_train = len(train_Data)\n",
    "n_valid = len(val_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "419c0160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_transform =Compose([\n",
    "    LoadImaged(keys=[\"t1\",\"tof\"]),\n",
    "    EnsureChannelFirstd(keys=[\"t1\",\"tof\"]),\n",
    "    # RandGaussianSmoothd(\n",
    "    #     keys=[\"t1\",\"tof\"],\n",
    "    #     sigma_x=(0.25, 1.5),\n",
    "    #     sigma_y=(0.25, 1.5),\n",
    "    #     sigma_z=(0.25, 1.5),\n",
    "    #     approx=\"erf\",\n",
    "    #     prob=0.3,\n",
    "    #     allow_missing_keys=False,\n",
    "    # ),\n",
    "    # RandGaussianNoised(keys=[\"t1\",\"tof\"],\n",
    "    #                        prob=0.5, mean=0, std=25, \n",
    "    #                        allow_missing_keys=False), \n",
    "    CenterSpatialCropd(\n",
    "        keys=[\"t1\",\"tof\"],\n",
    "        roi_size=(164,192,164)  \n",
    "    ),\n",
    "    RandSpatialCropd(keys=[\"t1\",\"tof\"],\n",
    "                         roi_size=(160,192,160), \n",
    "                         max_roi_size=None, \n",
    "                         random_center=True, \n",
    "                         random_size=False\n",
    "                        ),\n",
    "    # RandFlipd(\n",
    "    #         keys=[\"t1\",\"tof\"],\n",
    "    #         spatial_axis=[0],\n",
    "    #         prob=0.5,\n",
    "    #     ),\n",
    "    ToTensord(\n",
    "        keys=[\"t1\",\"tof\"]\n",
    "    ),\n",
    "])\n",
    "\n",
    "valid_transform = Compose([\n",
    "    LoadImaged(keys=[\"t1\",\"tof\", \"mask\"]),\n",
    "    EnsureChannelFirstd(keys=[\"t1\",\"tof\",\"mask\"]),\n",
    "    CenterSpatialCropd(\n",
    "        keys=[\"t1\",\"tof\",\"mask\"],\n",
    "        roi_size=(160,192,160)  \n",
    "    ),\n",
    "    ToTensord(\n",
    "        keys=[\"t1\",\"tof\",\"mask\"]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3976eac5-f941-4efc-a66f-1b167382a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(\n",
    "    data=train_Data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24824c9c-c8d2-418a-b846-1fafa413dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = Dataset(\n",
    "    data=val_Data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea49d0f-88fe-4069-8ed2-4328b38d0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = Dataset(\n",
    "    data=data_dict,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "all_loader = DataLoader(\n",
    "    all_ds, batch_size=1, shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76eaf4df-9ed1-4023-866c-42275235e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_t1_tof_strange_idx = [55] # combine 결과 값이 실제 label 이상인 경우\n",
    "up_t1_strange_idx = [149, 202] # t1 model 결과 값이 실제 label 이상인 경우\n",
    "up_tof_strange_idx = [0, 6, 7, 14, 25, 44, 52, 55, 64, 67, 147] # tof mdoel 결과 실제 label 이상인 경우\n",
    "\n",
    "down_t1_tof_strange_idx = [10, 46, 300, 316, 410]\n",
    "down_t1_strange_idx = [46, 148, 253, 269, 295, 296, 314, 316, 322, 366]\n",
    "down_tof_strange_idx = [10, 46, 59, 121, 142, 316, 410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f8ba1bd-c644-44c0-80e8-d56022da9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_indices(data_dict, indices):\n",
    "    filtered_data = [data_dict[i] for i in indices]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a5f8d61-fecd-4f26-96f7-232eee090f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 11\n",
      "5 10 7\n"
     ]
    }
   ],
   "source": [
    "up_t1_tof_strange_data = filter_data_by_indices(data_dict, up_t1_tof_strange_idx)\n",
    "up_t1_strange_data = filter_data_by_indices(data_dict, up_t1_strange_idx)\n",
    "up_tof_strange_data = filter_data_by_indices(data_dict, up_tof_strange_idx)\n",
    "\n",
    "down_t1_tof_strange_data = filter_data_by_indices(data_dict, down_t1_tof_strange_idx)\n",
    "down_t1_strange_data = filter_data_by_indices(data_dict, down_t1_strange_idx)\n",
    "down_tof_strange_data = filter_data_by_indices(data_dict, down_tof_strange_idx)\n",
    "\n",
    "print(len(up_t1_tof_strange_data), len(up_t1_strange_data), len(up_tof_strange_data))\n",
    "print(len(down_t1_tof_strange_data), len(down_t1_strange_data), len(down_tof_strange_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "644deba7-a974-4c7d-86c8-7496396844a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_t1_tof_ds = Dataset(\n",
    "    data=up_t1_tof_strange_data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "up_t1_tof_loader = DataLoader(\n",
    "    up_t1_tof_ds, batch_size=1, shuffle=False, \n",
    ")\n",
    "up_t1_ds = Dataset(\n",
    "    data=up_t1_strange_data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "up_t1_loader = DataLoader(\n",
    "    up_t1_ds, batch_size=1, shuffle=False, \n",
    ")\n",
    "up_tof_ds = Dataset(\n",
    "    data=up_tof_strange_data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "up_tof_loader = DataLoader(\n",
    "    up_tof_ds, batch_size=1, shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fe31351-eb36-4366-8197-955bd93a526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "down_t1_tof_ds = Dataset(\n",
    "    data=down_t1_tof_strange_data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "down_t1_tof_loader = DataLoader(\n",
    "    down_t1_tof_ds, batch_size=1, shuffle=False, \n",
    ")\n",
    "down_t1_ds = Dataset(\n",
    "    data=down_t1_strange_data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "down_t1_loader = DataLoader(\n",
    "    down_t1_ds, batch_size=1, shuffle=False, \n",
    ")\n",
    "down_tof_ds = Dataset(\n",
    "    data=down_tof_strange_data,\n",
    "    transform=valid_transform,\n",
    ")\n",
    "down_tof_loader = DataLoader(\n",
    "    down_tof_ds, batch_size=1, shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36b1b42d-b2b8-465f-9c94-6e625a7f6874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([74])\n",
      "tensor([52])\n",
      "tensor([51])\n",
      "tensor([83])\n",
      "tensor([63])\n",
      "tensor([86])\n",
      "tensor([83])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(down_tof_loader):\n",
    "    labels = batch[\"label\"]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "126ee5e8-827b-433e-8d8f-7a9549381cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiSFCN(nn.Module):\n",
    "    def __init__(self, channel_number=[32, 64, 128, 256, 256, 64], output_dim=40, dropout=True):\n",
    "        super(MultiSFCN, self).__init__()\n",
    "        n_layer = len(channel_number)\n",
    "        \n",
    "        # Define encoders for each contrast\n",
    "        self.encoder1 = self._make_encoder(channel_number)\n",
    "        self.encoder2 = self._make_encoder(channel_number)\n",
    "        \n",
    "        # Classifier\n",
    "        self.x1_classifier = self._make_classifier(channel_number[-1], output_dim, dropout)\n",
    "        self.x2_classifier = self._make_classifier(channel_number[-1], output_dim, dropout)\n",
    "        self.classifier = self._make_classifier(128, output_dim, dropout)  # Multiply by 2 for concatenated features\n",
    "\n",
    "    def _make_encoder(self, channel_number):\n",
    "        encoder = nn.Sequential()\n",
    "        n_layer = len(channel_number)\n",
    "        for i in range(n_layer):\n",
    "            if i == 0:\n",
    "                in_channel = 1  # Input channels for the first layer\n",
    "            else:\n",
    "                in_channel = channel_number[i-1]  # Input channels for subsequent layers\n",
    "            out_channel = channel_number[i]\n",
    "            if i < n_layer-1:\n",
    "                encoder.add_module('conv_%d' % i, self.conv_layer(in_channel,\n",
    "                                                                       out_channel,\n",
    "                                                                       maxpool=True,\n",
    "                                                                       kernel_size=3,\n",
    "                                                                       padding=1))\n",
    "            else:\n",
    "                encoder.add_module('conv_%d' % i, self.conv_layer(in_channel,\n",
    "                                                                       out_channel,\n",
    "                                                                       maxpool=False,\n",
    "                                                                       kernel_size=1,\n",
    "                                                                       padding=0))\n",
    "        return encoder\n",
    "    def _upsample(self, in_channels, out_channels):\n",
    "        # Define upsampling layers\n",
    "        upsample = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=40, out_channels=64, kernel_size=(5,6,5), stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Apply upsampling\n",
    "        return upsample\n",
    "        \n",
    "    def _make_combine_conv(self, in_channels, out_channels):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        return layer\n",
    "        \n",
    "    def _make_classifier(self, in_channels, output_dim, dropout):\n",
    "        classifier = nn.Sequential()\n",
    "        avg_shape = [5, 6, 5]\n",
    "        classifier.add_module('average_pool', nn.AvgPool3d(avg_shape))\n",
    "        if dropout is True:\n",
    "            classifier.add_module('dropout', nn.Dropout(0.5))\n",
    "        classifier.add_module('conv', nn.Conv3d(in_channels, output_dim, padding=0, kernel_size=1))\n",
    "        return classifier\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_layer(in_channel, out_channel, maxpool=True, kernel_size=3, padding=0, maxpool_stride=2):\n",
    "        if maxpool is True:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),\n",
    "                nn.BatchNorm3d(out_channel),\n",
    "                nn.MaxPool3d(2, stride=maxpool_stride),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),\n",
    "                nn.BatchNorm3d(out_channel),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through each encoder\n",
    "        x1 = x[:, 0, ...]\n",
    "        x2 = x[:, 1, ...]\n",
    "        x1_f = self.encoder1(x1) #[bs, 64, 5, 6, 5]\n",
    "        x2_f = self.encoder2(x2)\n",
    "\n",
    "        combined_features = torch.cat((x1_f, x2_f), dim=1) # [bs, 128, 5, 6, 5]\n",
    "\n",
    "        output = self.classifier(combined_features) # [bs, 40, 1, 1, 1]\n",
    "        x1_out = self.x1_classifier(x1_f)\n",
    "        x2_out = self.x2_classifier(x2_f)\n",
    "        \n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        x1_out = F.log_softmax(x1_out, dim=1)\n",
    "        x2_out = F.log_softmax(x2_out, dim=1)\n",
    "        \n",
    "        return output, x1_out, x2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff1e3afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU CHECK : True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MultiSFCN(\n",
       "    (encoder1): Sequential(\n",
       "      (conv_0): Sequential(\n",
       "        (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_1): Sequential(\n",
       "        (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_2): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_3): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_4): Sequential(\n",
       "        (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_5): Sequential(\n",
       "        (0): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (encoder2): Sequential(\n",
       "      (conv_0): Sequential(\n",
       "        (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_1): Sequential(\n",
       "        (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_2): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_3): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_4): Sequential(\n",
       "        (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (conv_5): Sequential(\n",
       "        (0): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (x1_classifier): Sequential(\n",
       "      (average_pool): AvgPool3d(kernel_size=[5, 6, 5], stride=[5, 6, 5], padding=0)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (conv): Conv3d(64, 40, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "    (x2_classifier): Sequential(\n",
       "      (average_pool): AvgPool3d(kernel_size=[5, 6, 5], stride=[5, 6, 5], padding=0)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (conv): Conv3d(64, 40, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (average_pool): AvgPool3d(kernel_size=[5, 6, 5], stride=[5, 6, 5], padding=0)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (conv): Conv3d(128, 40, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"GPU CHECK : {True if device.type == 'cuda' else False}\")\n",
    "\n",
    "model = MultiSFCN()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0e8e0f4-5998-47de-af96-d2bfba22823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_range = [12,92]\n",
    "bin_step = 2\n",
    "sigma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b528af43-e151-44c5-8466-87890dc502f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [20,80]\n",
    "y, bc = dpu.num2vect(label, bin_range, bin_step, sigma)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53e87eeb-9b2f-4a6c-ad3c-c0a442a71084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./new_multi_sfcn/1/models/model_4.699.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdbb3cb-c351-4310-8d42-1cb525d67086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_rows = 6\n",
    "num_cols = 6\n",
    "num_slices = num_rows * num_cols\n",
    "\n",
    "threshold_value = 0.1\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(num_slices):\n",
    "    slice_num = i * 5\n",
    "\n",
    "    if slice_num > 155:\n",
    "        break\n",
    "\n",
    "    rotated_x = np.rot90(x1[0, 0, :, :, slice_num], k=1)\n",
    "    rotated_mask = np.rot90(mask[0, 0, :, :, slice_num], k=1)\n",
    "    rotated_cam = np.rot90(final_grayscale_cam[0, :, :, slice_num], k=1)\n",
    "    rotated_cam_threshold = np.where(rotated_cam > threshold_value, 100, 0)\n",
    "\n",
    "    overlapping_indices = (rotated_cam_threshold == 100)\n",
    "    \n",
    "    overlapping_mask_values = rotated_mask.copy()\n",
    "    overlapping_mask_values[~overlapping_indices] = 0\n",
    "\n",
    "    if overlapping_mask_values.size > 0:\n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        plt.imshow(rotated_x, cmap='gray')\n",
    "        plt.imshow(overlapping_mask_values, cmap=\"jet\",alpha=0.2)\n",
    "        plt.title(f'Slice {slice_num}')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab6cfd-3d08-4368-8fa1-fdd1c780be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "nifti_path = '/mnt/babymri/data_age_mni1mm/IXI002-Guys-0828/T1_bet.nii.gz'\n",
    "nft = nib.load(nifti_path)\n",
    "data = nft.get_fdata()\n",
    "\n",
    "affine=nft.affine\n",
    "header=nft.header\n",
    "\n",
    "th = 0.1\n",
    "\n",
    "mask_tmp = torch.tensor(mask[0, 0, :, :, :])\n",
    "cam_tmp = torch.tensor(final_grayscale_cam[0])\n",
    "cam_threshold = np.where(cam_tmp > th, 1, 0)\n",
    "\n",
    "overlapping_indices = (cam_threshold == 1)\n",
    "\n",
    "overlapping_masks = mask_tmp\n",
    "overlapping_masks[~overlapping_indices] = 0\n",
    "\n",
    "print(overlapping_masks.shape)\n",
    "\n",
    "mask_values = overlapping_masks.flatten()\n",
    "unique_values = np.unique(mask_values.cpu())\n",
    "\n",
    "print(\"mask 배열의 값들:\")\n",
    "print(unique_values)\n",
    "\n",
    "nib.save(nib.Nifti1Image(overlapping_masks.cpu(), affine=affine, header=header), './GradCam/T1/60_T1_freesurf_old_segmask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a2b86-bad7-43e0-b2dc-fe103efbe7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "cam_nft = nib.load('./GradCam/T1/T1_20_final_grayscale_cam.nii.gz')\n",
    "final_grayscale_cam = cam_nft.get_fdata()\n",
    "\n",
    "print(final_grayscale_cam.shape)\n",
    "x1 = nib.load('./T1_bet_cropped.nii.gz')\n",
    "x1 = x1.get_fdata()\n",
    "print(x1.shape)\n",
    "\n",
    "for batch in train_loader:\n",
    "    x2, mask, age = (batch[\"tof\"]).to(device), (batch[\"mask\"]).to(device), batch[\"label\"]\n",
    "    break\n",
    "\n",
    "# mask = nib.load('./GradCam/T1/20_T1_freesurf_segmask.nii.gz')\n",
    "# mask = mask.get_fdata()\n",
    "# print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df80f6-8075-478a-9ac1-f688c72a5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 0.095\n",
    "\n",
    "rotated_x = np.rot90(x1[:, :, 50], k=1)\n",
    "rotated_mask = np.rot90(mask[0,0,:, :, 50], k=1)\n",
    "rotated_cam = np.rot90(final_grayscale_cam[:, :, 50], k=1)\n",
    "rotated_cam_threshold = np.where(rotated_cam < threshold_value, 0, rotated_cam)\n",
    "\n",
    "overlapping_indices = (rotated_mask != 0) & (rotated_cam_threshold != 0)\n",
    "\n",
    "masked_cam = np.ma.masked_where(~overlapping_indices, rotated_cam)\n",
    "\n",
    "plt.imshow(rotated_x, cmap='gray')\n",
    "plt.imshow(masked_cam, cmap='jet', alpha=0.2)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089c889-ac7d-4471-a9fd-558c0b90206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 0.1\n",
    "\n",
    "rotated_x = np.rot90(x1[:, 65, :], k=1)\n",
    "rotated_mask = np.rot90(mask[0,0,:, 65, :], k=1)\n",
    "rotated_cam = np.rot90(final_grayscale_cam[:, 65, :], k=1)\n",
    "rotated_cam_threshold = np.where(rotated_cam < threshold_value, 0, rotated_cam)\n",
    "\n",
    "overlapping_indices = (rotated_mask != 0) & (rotated_cam_threshold != 0)\n",
    "\n",
    "masked_cam = np.ma.masked_where(~overlapping_indices, rotated_cam)\n",
    "\n",
    "plt.imshow(rotated_x, cmap='gray')\n",
    "plt.imshow(masked_cam, cmap='jet', alpha=0.2)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ef9eb-4dca-4bd3-b3c5-8c2fb702540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 0.1\n",
    "\n",
    "rotated_x = np.rot90(x1[75, :, :], k=1)\n",
    "rotated_mask = np.rot90(mask[0,0,75, :, :], k=1)\n",
    "rotated_cam = np.rot90(final_grayscale_cam[75, :, :], k=1)\n",
    "rotated_cam_threshold = np.where(rotated_cam < threshold_value, 0, rotated_cam)\n",
    "\n",
    "overlapping_indices = (rotated_mask != 0) & (rotated_cam_threshold != 0)\n",
    "\n",
    "masked_cam = np.ma.masked_where(~overlapping_indices, rotated_cam)\n",
    "\n",
    "plt.imshow(rotated_x, cmap='gray')\n",
    "plt.imshow(masked_cam, cmap='jet', alpha=0.2)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d12906-4a0f-4bd7-9cad-1e3e1080343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_rows = 6\n",
    "num_cols = 6\n",
    "num_slices = num_rows * num_cols\n",
    "\n",
    "threshold_value = 0.095\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(num_slices):\n",
    "    slice_num = i * 5\n",
    "\n",
    "    if slice_num > 155:\n",
    "        break\n",
    "\n",
    "    rotated_x = np.rot90(x1[:, :, slice_num], k=1)\n",
    "    rotated_mask = np.rot90(mask[0,0,:, :, slice_num], k=1)\n",
    "    rotated_cam = np.rot90(final_grayscale_cam[:, :, slice_num], k=1)\n",
    "    rotated_cam_threshold = np.where(rotated_cam < threshold_value, 0, rotated_cam)\n",
    "\n",
    "    overlapping_indices = (rotated_mask != 0) & (rotated_cam_threshold != 0)\n",
    "    \n",
    "    masked_cam = np.ma.masked_where(~overlapping_indices, rotated_cam)\n",
    "\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(rotated_x, cmap='gray')\n",
    "    plt.imshow(masked_cam, cmap='jet', alpha=0.2)\n",
    "    plt.title(f'Slice {slice_num}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb6c16-ed85-4d1e-815b-e696e37833f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_rows = 7\n",
    "num_cols = 7\n",
    "num_slices = num_rows * num_cols\n",
    "\n",
    "threshold_value = 0.1\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(num_slices):\n",
    "    slice_num = 10 + i * 5\n",
    "\n",
    "    if slice_num > 190:\n",
    "        break\n",
    "    \n",
    "    rotated_x = np.rot90(x1[:, slice_num, :], k=1)\n",
    "    rotated_mask = np.rot90(mask[0, 0, :, slice_num, :], k=1)\n",
    "    rotated_cam = np.rot90(final_grayscale_cam[:, slice_num, :], k=1)\n",
    "    rotated_cam_threshold = np.where(rotated_cam < threshold_value, 0, rotated_cam)\n",
    "    \n",
    "    overlapping_indices = (rotated_mask != 0) & (rotated_cam_threshold != 0)\n",
    "    \n",
    "    masked_cam = np.ma.masked_where(~overlapping_indices, rotated_cam)\n",
    "    \n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(rotated_x, cmap='gray')\n",
    "    plt.imshow(masked_cam, cmap='jet', alpha=0.2)\n",
    "    plt.title(f'Slice {slice_num}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704ecd0-5dea-4e3e-b8ea-75fc38203260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_rows = 6\n",
    "num_cols = 6\n",
    "num_slices = num_rows * num_cols\n",
    "\n",
    "threshold_value = 0.1\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(num_slices):\n",
    "    slice_num = i * 5\n",
    "\n",
    "    if slice_num > 155:\n",
    "        break\n",
    "    \n",
    "    rotated_x = np.rot90(x1[slice_num, :, :], k=1)\n",
    "    rotated_mask = np.rot90(mask[0, 0, slice_num, :, :], k=1)\n",
    "    rotated_cam = np.rot90(final_grayscale_cam[slice_num, :, :], k=1)\n",
    "    rotated_cam_threshold = np.where(rotated_cam < threshold_value, 0, rotated_cam)\n",
    "    \n",
    "    overlapping_indices = (rotated_mask != 0) & (rotated_cam_threshold != 0)\n",
    "    \n",
    "    masked_cam = np.ma.masked_where(~overlapping_indices, rotated_cam)\n",
    "    \n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(rotated_x, cmap='gray')\n",
    "    plt.imshow(masked_cam, cmap='jet', alpha=0.2)\n",
    "    plt.title(f'Slice {slice_num}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1bdf6-ac93-4cfc-8571-539bcc61d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "nifti_path = '/mnt/babymri/data_age_mni1mm/IXI002-Guys-0828/T1_bet.nii.gz'\n",
    "nft = nib.load(nifti_path)\n",
    "data = nft.get_fdata()\n",
    "\n",
    "affine=nft.affine\n",
    "header=nft.header\n",
    "\n",
    "nib.save(nib.Nifti1Image(final_grayscale_cam[0], affine=affine, header=header), './GradCam/T1/T1_old_train_final_grayscale_cam.nii.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb305b64-2676-4eeb-9039-f5aba85b476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_grayscale_cam[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c02722-2a57-47d5-bf71-eb357dfcd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "nifti_path = '/mnt/babymri/data_age_mni1mm/IXI002-Guys-0828/T1_bet.nii.gz'\n",
    "nft = nib.load(nifti_path)\n",
    "data = nft.get_fdata()\n",
    "print(data.shape)\n",
    "affine=nft.affine\n",
    "header=nft.header\n",
    "\n",
    "# cropper = CenterSpatialCrop(roi_size=(160, 192, 160))\n",
    "# resampled_data = cropper(np.expand_dims(data, axis=0))\n",
    "\n",
    "# resampled_nifti = nib.Nifti1Image(resampled_data, affine=nft.affine, header=nft.header)\n",
    "# nib.save(resampled_nifti, 'T1_bet_cropped.nii.gz')\n",
    "\n",
    "tmp = nib.load('./GradCam/T1/T1_old_train_final_grayscale_cam.nii.gz')\n",
    "tmpd = tmp.get_fdata()\n",
    "\n",
    "threshold_value = 0.07\n",
    "mask = np.where(tmpd > threshold_value, 1, 0)\n",
    "print(mask.shape)\n",
    "\n",
    "nib.save(nib.Nifti1Image(mask, affine=affine, header=header), './GradCam/T1/T1_old_train_cam_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9416383-bf90-48ed-8032-5f661342295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "nifti_path = '/mnt/babymri/data_age_mni1mm/IXI002-Guys-0828/T1_bet.nii.gz'\n",
    "nft = nib.load(nifti_path)\n",
    "data = nft.get_fdata()\n",
    "print(data.shape)\n",
    "affine=nft.affine\n",
    "header=nft.header\n",
    "\n",
    "# cropper = CenterSpatialCrop(roi_size=(160, 192, 160))\n",
    "# resampled_data = cropper(np.expand_dims(data, axis=0))\n",
    "\n",
    "# resampled_nifti = nib.Nifti1Image(resampled_data, affine=nft.affine, header=nft.header)\n",
    "# nib.save(resampled_nifti, 'T1_bet_cropped.nii.gz')\n",
    "\n",
    "tmp = nib.load('./GradCam/T1/T1_young_train_final_grayscale_cam.nii.gz')\n",
    "tmpd = tmp.get_fdata()\n",
    "\n",
    "threshold_value = 0.07\n",
    "mask = np.where(tmpd > threshold_value, 1, 0)\n",
    "\n",
    "nib.save(nib.Nifti1Image(mask, affine=affine, header=header), './GradCam/T1/T1_young_train_cam_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9fef8-a8eb-4e1d-948f-4765346ec1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import nibabel as nib\n",
    "\n",
    "# m1 = nib.load('./20_cam_mask.nii.gz')\n",
    "# m2 = nib.load('./40_cam_mask.nii.gz')\n",
    "# m3 = nib.load('./60_cam_mask.nii.gz')\n",
    "\n",
    "# affine = m1.affine\n",
    "# header = m1.header\n",
    "\n",
    "# data1 = m1.get_fdata()\n",
    "# data2 = m2.get_fdata()\n",
    "# data3 = m3.get_fdata()\n",
    "\n",
    "# total_mask_data = np.zeros_like(data1)\n",
    "\n",
    "# total_mask_data[data1 > 0] = 1\n",
    "# total_mask_data[data2 > 0] = 2\n",
    "# total_mask_data[data3 > 0] = 3\n",
    "\n",
    "# nib.save(nib.Nifti1Image(total_mask_data, affine=affine, header=header), 'total_mask.nii.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0d3dd-1c6b-4b95-9c86-b05370b2c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "m1 = nib.load('./GradCam/T1/T1_young_train_cam_mask.nii.gz')\n",
    "m2 = nib.load('./GradCam/T1/T1_old_train_cam_mask.nii.gz')\n",
    "\n",
    "affine = m1.affine\n",
    "header = m1.header\n",
    "\n",
    "data1 = m1.get_fdata()\n",
    "print(data1.shape)\n",
    "data2 = m2.get_fdata()\n",
    "print(data2.shape)\n",
    "\n",
    "total_mask_data = np.zeros_like(data1)\n",
    "\n",
    "total_mask_data[data1 > 0] = 1\n",
    "total_mask_data[data2 > 0] = 2\n",
    "\n",
    "nib.save(nib.Nifti1Image(total_mask_data, affine=affine, header=header), './GradCam/T1/T1_train_two_range_mask.nii.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bd4f1-64cb-47ce-a4ec-a7d2bcf17673",
   "metadata": {},
   "source": [
    "## Volume 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c9340007-b526-454a-a7c5-92c2ff270f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "down_vol_idx = [300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ce8316aa-8116-4aef-bb4a-bf4b345f2a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2888380/1737483120.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_tmp = torch.tensor(mask[0, 0, :, :, :])\n",
      "/tmp/ipykernel_2888380/1737483120.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_tmp = torch.tensor(mask[0, 0, :, :, :])\n"
     ]
    }
   ],
   "source": [
    "strange_vol_list = []\n",
    "strange_age_list = []\n",
    "strange_img_list = []\n",
    "\n",
    "vol_list = []\n",
    "age_list = []\n",
    "img_list = []\n",
    "\n",
    "left_label_list = [17, 10, 8, 3]\n",
    "right_label_list = [53, 49, 47, 42]\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for step, batch in enumerate(all_loader):\n",
    "    x1, x2, mask, age = (batch[\"t1\"]).to(device), (batch[\"tof\"]).to(device), (batch[\"mask\"]).to(device), batch[\"label\"]\n",
    "    img_name = data_dict[step]['t1']\n",
    "    nft = nib.load(img_name)\n",
    "    \n",
    "    affine=nft.affine\n",
    "    header=nft.header\n",
    "    voxel_size = nft.header.get_zooms()\n",
    "    if step in down_vol_idx:\n",
    "        x1 = x1 / x1.mean()\n",
    "        x2 = x2 / x2.mean()\n",
    "        x = torch.stack((x1, x2), dim=1)\n",
    "\n",
    "        mask_tmp = torch.tensor(mask[0, 0, :, :, :])\n",
    "\n",
    "        left_vol = 0.\n",
    "        right_vol = 0.\n",
    "        tmp_vol_list = []\n",
    "        \n",
    "        for left_label, right_label in zip(left_label_list, right_label_list):\n",
    "            left_mask = np.zeros_like(mask_tmp.cpu())\n",
    "            left_mask[(mask_tmp.cpu()==left_label)] = 1\n",
    "            left_vol = left_mask.sum() * voxel_size[0] * voxel_size[1] * voxel_size[2]\n",
    "            right_mask = np.zeros_like(mask_tmp.cpu())\n",
    "            right_mask[(mask_tmp.cpu()==right_label)] = 1\n",
    "            right_vol = right_mask.sum() * voxel_size[0] * voxel_size[1] * voxel_size[2]\n",
    "            total_vol = left_vol + right_vol\n",
    "            tmp_vol_list.append(total_vol)\n",
    "    \n",
    "        strange_vol_list.append(tmp_vol_list)\n",
    "        strange_age_list.append(age)\n",
    "        strange_img_list.append(img_name)\n",
    "    else:\n",
    "        if (age >= 38 and age <= 48):\n",
    "            x1 = x1 / x1.mean()\n",
    "            x2 = x2 / x2.mean()\n",
    "            x = torch.stack((x1, x2), dim=1)\n",
    "    \n",
    "            mask_tmp = torch.tensor(mask[0, 0, :, :, :])\n",
    "    \n",
    "            left_vol = 0.\n",
    "            right_vol = 0.\n",
    "            tmp_vol_list = []\n",
    "        \n",
    "            for left_label, right_label in zip(left_label_list, right_label_list):\n",
    "                left_mask = np.zeros_like(mask_tmp.cpu())\n",
    "                left_mask[(mask_tmp.cpu()==left_label)] = 1\n",
    "                left_vol = left_mask.sum() * voxel_size[0] * voxel_size[1] * voxel_size[2]\n",
    "                right_mask = np.zeros_like(mask_tmp.cpu())\n",
    "                right_mask[(mask_tmp.cpu()==right_label)] = 1\n",
    "                right_vol = right_mask.sum() * voxel_size[0] * voxel_size[1] * voxel_size[2]\n",
    "                total_vol = left_vol + right_vol\n",
    "                tmp_vol_list.append(total_vol)\n",
    "\n",
    "            vol_list.append(tmp_vol_list)\n",
    "            age_list.append(age)\n",
    "            img_list.append(img_name)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1033159d-b026-4996-b659-4481714c618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11516.0, 20709.0, 128828.0, 673100.0]]\n",
      "79\n",
      "11276.962025316456 18799.354430379746 134308.48101265822 639212.7974683545\n",
      "801928.0 773521.2784810127\n"
     ]
    }
   ],
   "source": [
    "print(strange_vol_list)\n",
    "hippo = 0\n",
    "thalamus = 0\n",
    "cortex_small = 0\n",
    "cortex_big = 0\n",
    "for li in vol_list:\n",
    "    hippo += li[0]\n",
    "    thalamus += li[1]\n",
    "    cortex_small += li[2]\n",
    "    cortex_big += li[3]\n",
    "length = len(vol_list)\n",
    "print(length)\n",
    "print(hippo/length, thalamus/length, cortex_small/length, cortex_big/length)\n",
    "print(strange_vol_list[0][2] + strange_vol_list[0][3], cortex_small/length + cortex_big/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c21391-89bc-4962-96bb-9c6ec7680da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_list = [int(age.item()) for age in age_list]\n",
    "print(age_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b1011-7129-4a2d-98cd-680c5125d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(left_list[2])\n",
    "print(right_list[2])\n",
    "print(vol_list[2])\n",
    "print(age_list[2])\n",
    "print(img_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38921f11-2278-402f-9059-b213fe6c6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vol_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba3107-b180-46ec-8c62-6a27a763e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(age_list, img_list, vol_list))\n",
    "\n",
    "combined.sort()\n",
    "\n",
    "sorted_age_list, sorted_img_list, sorted_vol_list = zip(*combined)\n",
    "\n",
    "sorted_age_list = list(sorted_age_list)\n",
    "sorted_img_list = list(sorted_img_list)\n",
    "sorted_vol_list = list(sorted_vol_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22bf48-9deb-4d88-95c4-4dd995b18349",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img_list = []\n",
    "for path in sorted_img_list:\n",
    "    target_part = path.split('/')[-2]\n",
    "    new_img_list.append(target_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fef0d8-d8bb-49b0-bcc5-2746befda38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 프레임 생성\n",
    "data = {\n",
    "    'age': sorted_age_list,\n",
    "    'cortex_vol': sorted_vol_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# new_img_list를 인덱스로 설정\n",
    "df.set_index(pd.Index(new_img_list), inplace=True)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "df.to_csv('./Cortex_sorted_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9df558-f916-47ca-a5db-927c589a9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv(\"hippo_sorted_data.csv\")\n",
    "\n",
    "# age와 hippocampus_vol 열을 추출하여 리스트에 저장\n",
    "sorted_age_list = df['age'].tolist()\n",
    "sorted_vol_list = df['hippocampus_vol'].tolist()\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Sorted Age List:\", sorted_age_list)\n",
    "print(\"Sorted Volume List:\", sorted_vol_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01e4b4-e741-41f8-a88f-68e984ce2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Scatter plot 그리기\n",
    "plt.scatter(sorted_age_list, sorted_vol_list, label='Volume')\n",
    "\n",
    "# Volume 변화량 계산\n",
    "volume_changes = [sorted_vol_list[i+1] - sorted_vol_list[i] for i in range(len(sorted_vol_list)-1)]\n",
    "\n",
    "# Age와 Volume 변화량으로 선형 근사 계산\n",
    "x = np.array(sorted_age_list[:-1])\n",
    "y = np.array(volume_changes)\n",
    "p = np.polyfit(sorted_age_list, sorted_vol_list, 1)\n",
    "\n",
    "# Age 범위 생성\n",
    "x_range = np.linspace(min(x), max(x), 100)\n",
    "\n",
    "# 선형 근사에 따른 y 값 생성\n",
    "y_fit = np.polyval(p, x_range)\n",
    "\n",
    "# 부드럽게 그려진 선 그래프 추가\n",
    "plt.plot(x_range, y_fit, '-r', label='Trendline')\n",
    "\n",
    "# 그래프 제목, 라벨 등 추가\n",
    "plt.title('Hippo Volumes')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Volume')\n",
    "plt.legend()\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c672a-e541-4f67-8c22-405126bab280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기 (파일 이름을 \"data.csv\"로 가정)\n",
    "df = pd.read_csv('./Volume_sort/hippo_sorted_data.csv')\n",
    "\n",
    "# 각 연령대별 부피의 평균값을 계산\n",
    "age_20_39 = df[(df['age'] >= 20) & (df['age'] <= 39)]['hippocampus_vol'].mean()\n",
    "age_40_59 = df[(df['age'] >= 40) & (df['age'] <= 59)]['hippocampus_vol'].mean()\n",
    "age_60_plus = df[df['age'] >= 60]['hippocampus_vol'].mean()\n",
    "\n",
    "# 평균값 출력\n",
    "print(f\"20-39세의 부피 평균값: {age_20_39}\")\n",
    "print(f\"40-59세의 부피 평균값: {age_40_59}\")\n",
    "print(f\"60세 이상의 부피 평균값: {age_60_plus}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d322edb-3fa8-47e4-8e50-324317cda0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기 (파일 이름을 \"data.csv\"로 가정)\n",
    "df = pd.read_csv('./Volume_sort/thalamus_sorted_data.csv')\n",
    "\n",
    "# 각 연령대별 부피의 평균값을 계산\n",
    "age_20_39 = df[(df['age'] >= 20) & (df['age'] <= 39)]['thalamus_vol'].mean()\n",
    "age_40_59 = df[(df['age'] >= 40) & (df['age'] <= 59)]['thalamus_vol'].mean()\n",
    "age_60_plus = df[df['age'] >= 60]['thalamus_vol'].mean()\n",
    "\n",
    "# 평균값 출력\n",
    "print(f\"20-39세의 부피 평균값: {age_20_39}\")\n",
    "print(f\"40-59세의 부피 평균값: {age_40_59}\")\n",
    "print(f\"60세 이상의 부피 평균값: {age_60_plus}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b34be-de03-4ebf-81bf-75ce522be213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기 (파일 이름을 \"data.csv\"로 가정)\n",
    "df = pd.read_csv('./Volume_sort/Cortex_sorted_data.csv')\n",
    "\n",
    "# 각 연령대별 부피의 평균값을 계산\n",
    "age_20_39 = df[(df['age'] >= 20) & (df['age'] <= 39)]['cortex_vol'].mean()\n",
    "age_40_59 = df[(df['age'] >= 40) & (df['age'] <= 59)]['cortex_vol'].mean()\n",
    "age_60_plus = df[df['age'] >= 60]['cortex_vol'].mean()\n",
    "\n",
    "# 평균값 출력\n",
    "print(f\"20-39세의 부피 평균값: {age_20_39}\")\n",
    "print(f\"40-59세의 부피 평균값: {age_40_59}\")\n",
    "print(f\"60세 이상의 부피 평균값: {age_60_plus}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76103926-896c-4728-ad68-bbe50f2a2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기 (파일 이름을 \"data.csv\"로 가정)\n",
    "df = pd.read_csv('./Volume_sort/Cere_Cortex_sorted_data.csv')\n",
    "\n",
    "# 각 연령대별 부피의 평균값을 계산\n",
    "age_20_39 = df[(df['age'] >= 20) & (df['age'] <= 39)]['cortex_vol'].mean()\n",
    "age_40_59 = df[(df['age'] >= 40) & (df['age'] <= 59)]['cortex_vol'].mean()\n",
    "age_60_plus = df[df['age'] >= 60]['cortex_vol'].mean()\n",
    "\n",
    "# 평균값 출력\n",
    "print(f\"20-39세의 부피 평균값: {age_20_39}\")\n",
    "print(f\"40-59세의 부피 평균값: {age_40_59}\")\n",
    "print(f\"60세 이상의 부피 평균값: {age_60_plus}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695f785-e001-4bfa-aea0-8dfb0718395e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:med] *",
   "language": "python",
   "name": "conda-env-med-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
